{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8165591,"sourceType":"datasetVersion","datasetId":4831777}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === 1. SETUP ===\nimport pandas as pd\nimport numpy as np\nimport re\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import get_linear_schedule_with_warmup\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport os\nimport json\nimport random\nimport warnings\n\n# Suppress warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nwarnings.filterwarnings('ignore')\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# === 2. DATA LOADING & PREPROCESSING ===\ndef load_and_preprocess_data():\n    true_df = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\n    fake_df = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')\n\n    true_df['label'] = 0\n    fake_df['label'] = 1\n    df = pd.concat([true_df, fake_df], ignore_index=True)\n\n    def clean_text(text):\n        text = str(text).lower()\n        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n        text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n        return text\n\n    df['cleaned_text'] = df['text'].apply(clean_text)\n\n    print(\"\\nClass Distribution:\")\n    print(df['label'].value_counts())\n    return df\n\ndf = load_and_preprocess_data()\n\n# === 3. TRAIN-TEST SPLIT ===\nX_train, X_test, y_train, y_test = train_test_split(\n    df['cleaned_text'].values,\n    df['label'].values,\n    test_size=0.2,\n    random_state=42,\n    stratify=df['label']\n)\n\n# === 4. BERT TOKENIZATION ===\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\ndef tokenize_texts(texts, labels, max_len=128):\n    input_ids, attention_masks = [], []\n\n    for text in texts:\n        encoded = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=max_len,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n            truncation=True\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n\n    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0), torch.tensor(labels)\n\ntrain_inputs, train_masks, train_labels = tokenize_texts(X_train, y_train)\ntest_inputs, test_masks, test_labels = tokenize_texts(X_test, y_test)\n\n# === 5. DATALOADERS ===\nbatch_size = 32\ntrain_dataset = torch.utils.data.TensorDataset(train_inputs, train_masks, train_labels)\ntest_dataset = torch.utils.data.TensorDataset(test_inputs, test_masks, test_labels)\n\ntrain_dataloader = DataLoader(train_dataset, sampler=torch.utils.data.RandomSampler(train_dataset), batch_size=batch_size)\ntest_dataloader = DataLoader(test_dataset, sampler=torch.utils.data.SequentialSampler(test_dataset), batch_size=batch_size)\n\n# === 6. MODEL INITIALIZATION ===\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\nmodel.to(device)\n\n# === 7. TRAINING SETUP ===\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)\nepochs = 3\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n# === 8. TRAINING LOOP ===\ndef train_model():\n    model.train()\n    total_loss = 0\n\n    for batch in train_dataloader:\n        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n        model.zero_grad()\n        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n        loss = outputs.loss\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item()\n\n    print(f\"Average training loss: {total_loss / len(train_dataloader):.4f}\")\n\n# === 9. EVALUATION ===\ndef evaluate_model(dataloader):\n    model.eval()\n    predictions, true_labels = [], []\n\n    for batch in dataloader:\n        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        with torch.no_grad():\n            outputs = model(b_input_ids, attention_mask=b_input_mask)\n\n        logits = outputs.logits\n        predictions.append(logits.detach().cpu().numpy())\n        true_labels.append(b_labels.cpu().numpy())\n\n    predictions = np.concatenate(predictions, axis=0)\n    true_labels = np.concatenate(true_labels, axis=0)\n    pred_labels = np.argmax(predictions, axis=1)\n\n    from sklearn.metrics import classification_report\n    print(classification_report(true_labels, pred_labels))\n\n# === 10. CUSTOM TEST EVALUATION ===\ndef load_custom_test_data():\n    real_samples = [\n        \"The Prime Minister held a press conference to discuss new economic reforms.\",\n        ...  # 49 more realistic samples\n    ]\n    fake_samples = [\n        \"Aliens spotted participating in the Olympics disguised as athletes.\",\n        ...  # 49 more fake samples\n    ]\n    samples = [(text, 0) for text in real_samples] + [(text, 1) for text in fake_samples]\n    random.shuffle(samples)\n\n    with open(\"custom_test_samples.json\", \"w\") as f:\n        json.dump(samples, f)\n\n    return samples\n\ndef evaluate_custom_samples(samples):\n    model.eval()\n    correct = 0\n\n    print(f\"{'Sample':<3} | {'Prediction':<9} | {'Expected':<8} | {'Correct?':<8} | {'Confidence (Fake%)':<20}\")\n    print(\"-\" * 80)\n\n    for idx, (text, expected_label) in enumerate(samples):\n        encoded = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=128,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n            truncation=True\n        )\n\n        input_id = encoded['input_ids'].to(device)\n        attention_mask = encoded['attention_mask'].to(device)\n\n        with torch.no_grad():\n            outputs = model(input_id, attention_mask=attention_mask)\n\n        probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()[0]\n        pred_label = int(np.argmax(probs))\n        confidence = probs[1] * 100\n\n        is_correct = \"Yes\" if pred_label == expected_label else \"No\"\n        label_str = lambda x: \"Fake\" if x == 1 else \"Real\"\n\n        print(f\"{idx+1:<3} | {label_str(pred_label):<9} | {label_str(expected_label):<8} | {is_correct:<8} | {confidence:.2f}%\")\n\n        if pred_label == expected_label:\n            correct += 1\n\n    print(f\"\\nAccuracy on custom test set: {correct} / {len(samples)} = {correct / len(samples) * 100:.2f}%\")\n\n# === 11. MAIN EXECUTION ===\nif __name__ == '__main__':\n    train_model()\n    print(\"\\nEvaluation on Test Set:\")\n    evaluate_model(test_dataloader)\n\n    print(\"\\nEvaluation on Custom Sample Set:\")\n    custom_samples = load_custom_test_data()\n    evaluate_custom_samples(custom_samples)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:11:49.646589Z","iopub.execute_input":"2025-06-10T18:11:49.646840Z","iopub.status.idle":"2025-06-10T18:24:37.753898Z","shell.execute_reply.started":"2025-06-10T18:11:49.646817Z","shell.execute_reply":"2025-06-10T18:24:37.752845Z"}},"outputs":[{"name":"stderr","text":"2025-06-10 18:12:04.054133: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749579124.334184      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749579124.400099      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n\nClass Distribution:\nlabel\n1    23481\n0    21417\nName: count, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee37a5f4df1646cfa909d9b02bb6a733"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9fefdbc838e439888bc2ae3569305dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ae00ef942fe44f8987a005c0a24fa96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb5d5551a4234a55901a387214062f52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a782d5e5c4774b59855885597f357fbc"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 0.0168\n\nEvaluation on Test Set:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      4284\n           1       1.00      1.00      1.00      4696\n\n    accuracy                           1.00      8980\n   macro avg       1.00      1.00      1.00      8980\nweighted avg       1.00      1.00      1.00      8980\n\n\nEvaluation on Custom Sample Set:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2422392804.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEvaluation on Custom Sample Set:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mcustom_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_custom_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0mevaluate_custom_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2422392804.py\u001b[0m in \u001b[0;36mload_custom_test_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"custom_test_samples.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0m_floatstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Object of type ellipsis is not JSON serializable"],"ename":"TypeError","evalue":"Object of type ellipsis is not JSON serializable","output_type":"error"}],"execution_count":1}]}